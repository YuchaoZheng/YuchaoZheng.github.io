---
layout:     post
title:      Alexnet实现及微调
date:       2018-10-23
author:     ZYC
header-img: img/post-bg-ios9-web.jpg
catalog: 	 true
mathjax:     true
tags:
    - deeplearning
    - Alexnet
---

# 1.论文理解

## 1.1 数据集的处理

[参考链接:CNN在分类图片时图片大小不一怎么办](https://www.zhihu.com/question/45873400)

ImageNet包含各种清晰度的图片，而我们的系统要求输入维度恒定，因此，我们对图片进行采样，获得固定大小的256X256的分辨率。对于每张长方形的图，我们将短边按比例调整为256，然后取中心区域的256X256像素。(也有在全连接层加入SPP)

- - -
**标准化概念**
根据数据结构，把数据的值按行，按列，或者某些特征，或者某些属性
1. 统一映射到一个特定区间里，比如[-1,1]
2. 统一映射到某种特定分布里，比如矩阵为0,方差为1。

[参考链接：归一化原因](https://zhuanlan.zhihu.com/p/35597976)
**Dataset归一化**
具体做法就是对于整个训练集图片，每个通道分别减去训练集该通道平均值。

## 1.2 ReLu

**sigmoid 的两个主要缺点:**

- Sigmoid函数饱和使梯度消失。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。

- Sigmoid函数的输出不是零中心的。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在f=w^Tx+b中每个元素都x>0），那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式f而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。

[零中心解释](https://liam.page/2018/04/17/zero-centered-active-function/)
![](https://liam.page/uploads/images/MachineLearning/zig-zag-gradient.png)

** RELU **
- 优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ Krizhevsky 等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。
- 优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。
- 缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。(变为负数)例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。

# 2.Alexnet实现
[参考文章1](https://zhuanlan.zhihu.com/p/27381582)

## 2.1 Alexnet结构

- 网络结构



- 卷积层

由于2012年，GPU性能较弱，内存不够。因此AlexNet通过两个GPU协同训练CNN，在一些层的计算我们需要先将输入以及卷积核分为两组，分别计算然后得到的feature map再合并，因此我们设置了groups参数。



- maxPooling


- lrn

$b_{x,y}^{i} = a_{x,y}^{i}/(k+\alpha \sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^{i})^{2})^{\beta}$
$a[batch,\ height,\ weight,\ channels]$
$N是该层的feature\ map总数，n表示取该feature\ map为中间的左右各n/2个feature\ map来求均值。$
$k,n,\alpha,\beta都是固定值,每一层ReLU后面都接一层LRN。$
$a,n/2,k,α,β分别表示函数中的input,depth_radius,bias,alpha,beta$

- fc

- 创建网络图

- load_weights

- test_image

对于输入图像，我们首先将其大小归一化为网络输入大小，并定义一个占位符用以结果输出，然后加载网络模型，正向传播网络得到FC8层输出结果，然后将softmax结果转换为类别结果。



## 2.2 tensorflow运行流程
[参考文章](https://blog.csdn.net/u014595019/article/details/52677412)



# 3.Alexnext微调


augment 增加

1. Input layer: 224 * 224 * 3 （RGB图片，224 * 224像素）
2. Conv layer: 96 kernels of 11 * 11 * 3 - Response Normalization - Max-Pooling - ReLU
3. Conv layer2: 256 kernels of 5 * 5 * 48 - Response Normalization - Max-Pooling - ReLU
4. Conv layer3: 384 kernels of 3 * 3 * 256 - ReLU
5. Conv layer4: 384 kernels of 3 * 3 * 192 - Max-Pooling - ReLU
6. Conv layer5: 256 kernels of 3 * 3 * 192 - ReLU
7. Fully-connected1 of size 4096 - ReLU
8. Fully-connected2 of size 4096 - ReLU
9. Fully-connected3 of size 4096 - ReLU
10. Softmax output 1000


realistic 切实，实际
exhibit 表现出
variability 易变
shortcomings 缺点
immense complexity 非常复杂
compensate 补偿
assumptions 假设
statistics 统计
attractive 吸引力
prohibitively expensive 非常昂贵
scale 规模
paired with 配合
facilitate 促进
inferior 下，劣势
tolerate 容忍
variable-resolution  可变分辨率
rectangular 长方形



饱和非线性激活函数Relu比非饱和非线性慢得多。

GTX 580 GPU：内存3G
两个GPU并行
可以直接相互访问内存信息，不用通过服务器。

dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。

artificially enlarge  人工放大
augmentation  增强，放大
substantial	大量的

采取了两种不同形式的数据放大，它们都允许在仅对原图做少量计算的情况下产生变形的新图，所以变形后的新图无需存储在硬盘中。

在我们的实现中，转换的图像是在CPU上的Python代码中生成的，而GPU正在训练前一批图像。 因此，这些数据增强方案实际上在计算上是免费的。

第一种放大数据集（产生新图）的方式由图片平移和水平镜像组成，我们通过从256X256的图片中随机抽取224X224的区块（及其水平镜像）来实现这种方法，并在这些抽取后得到的区块上训练我们的神经网络。在测试过程中，网络会抽取五个（四角和中间）224 x 224的区块及其水平镜像进行预测，然后将softmax层对这十个区块做出的预测取平均。

第二种放大数据集的方法是对训练图片的RGB频谱密度进行改变。特别地，我们在整个ImageNet训练集上对RGB像素进行主成分分析(PCA)。

